ğŸ§  Day 15 â€“ Greedy Algorithm Practice & PCA (DSA + ML):

ğŸ“Œ SLOT 01: Greedy Algorithms (Practice Focus):

1ï¸âƒ£ What is Greedy Algorithm? (Recap)
Greedy algorithm ek aisa approach hai jisme har step par locally best choice li jati hai.

ğŸ‘‰ Simple words
Har step par jo best lagta hai wahi choose karo.

Example:
Coin change â†’ sabse bada coin pehle lo.

2ï¸âƒ£ Greedy Choice Property
Meaning:
Agar har step ka best decision future solution ko disturb nahi kare â†’ greedy work karega.

ğŸ‘‰ Important exam point â­
Greedy tab work karega jab local best = global best.

3ï¸âƒ£ When Greedy Works / Fails
âœ… Works:
Activity selection
Fractional knapsack
Huffman coding
âŒ Fails:
0/1 knapsack
Some coin change cases

ğŸ‘‰ Kyuki future impact ignore hota hai.

4ï¸âƒ£ Greedy vs Dynamic Programming
Greedy
ğŸ‘‰Dynamic Programming
ğŸ‘‰Local decision
ğŸ‘‰Global decision
ğŸ‘‰Fast
ğŸ‘‰Slow but optimal
ğŸ‘‰Easy
ğŸ‘‰Complex
ğŸ‘‰Not always correct
ğŸ‘‰Always optimal

ğŸ‘‰ Exam favourite comparison â­

5ï¸âƒ£ Coin Change (Greedy Idea)
Coins = [10, 5, 2, 1]
Amount = 18
ğŸ‘‰ Greedy solution
10 â†’ 5 â†’ 2 â†’ 1
ğŸ§‘â€ğŸ’» Code (concept)
Python
coins = [10,5,2,1]
amount = 18
count = 0

for coin in coins:
    count += amount // coin
    amount %= coin

print(count)

6ï¸âƒ£ Activity Selection Problem
ğŸ‘‰ Goal â†’ maximum activities choose without overlap
Logic:
Activity finish time sort
Jo earliest finish kare â†’ choose
ğŸ‘‰ Greedy works â­

7ï¸âƒ£ Fractional Knapsack
ğŸ‘‰ Item ko break kar sakte ho
ğŸ‘‰ Value/weight ratio highest â†’ pick first
ğŸ‘‰ Greedy works because fractional allowed.

ğŸ“Œ SLOT 02: PCA â€“ Principal Component Analysis:

1ï¸âƒ£ What is Dimensionality Reduction?
Features kam karna but information maintain karna.
ğŸ‘‰ Example
100 columns â†’ 10 columns
ğŸ‘‰ Model fast + overfitting kam

2ï¸âƒ£ Why PCA is Used
Feature reduction
Noise removal
Visualization
Faster ML training
ğŸ‘‰ Exam line â­

3ï¸âƒ£ Variance Concept
Variance = data spread
ğŸ‘‰ PCA high variance direction choose karta hai
Kyuki usme information jyada hoti hai.

4ï¸âƒ£ Eigenvector Idea (Simple)
ğŸ‘‰ Direction jisme data sabse jyada spread hai
ğŸ‘‰ PCA â†’ best direction find karta hai
Bas itna samajhna enough â­

5ï¸âƒ£ Feature Reduction Concept
ğŸ‘‰ PCA features combine karta hai
Example: Height + Weight + BMI
â†’ 1 component
ğŸ‘‰ Information compress ho jati hai.

6ï¸âƒ£ Mini PCA Example
Python
from sklearn.decomposition import PCA
import numpy as np

data = np.array([[2,3],[3,4],[4,5],[5,6]])

pca = PCA(n_components=1)
reduced = pca.fit_transform(data)

print(reduced)
ğŸ‘‰ 2D â†’ 1D convert

ğŸ” Quick Revision
ğŸ“ŒGreedy â†’ local best choice
ğŸ“ŒGreedy works when local = global
ğŸ“ŒFractional knapsack â†’ greedy
ğŸ“ŒDP â†’ global optimal
ğŸ“ŒPCA â†’ dimensionality reduction
ğŸ“ŒVariance â†’ information measure
ğŸ“ŒEigenvector â†’ max spread direction
ğŸ§ª Coding Practice Questions (with Answer)

âœ… Q1 Coin Change Greedy
Question: Count minimum coins for amount 27 (10,5,2,1)
Python
coins = [10,5,2,1]
amount = 27
count = 0

for coin in coins:
    count += amount // coin
    amount %= coin

print(count)
âœ… Q2 Activity Selection (Basic Logic)
Question: Print activities that can be selected
Python
start = [1,3,0,5]
finish = [2,4,6,7]

last_finish = 0

for i in range(len(start)):
    if start[i] >= last_finish:
        print(i)
        last_finish = finish[i]
âœ… Q3 PCA Reduction
Python
from sklearn.decomposition import PCA
import numpy as np

data = np.array([[1,2],[2,3],[3,4]])
pca = PCA(n_components=1)

print(pca.fit_transform(data))

Day 15 complete âœ…ğŸš€ğŸ˜(âÂ´â—¡`â)
