ğŸ§  Day 12 â€“ Sliding Window & Support Vector Machine (DSA + ML):

ğŸ“Œ SLOT 01: Sliding Window Technique (DSA):

1ï¸âƒ£ What is Sliding Window?
Definition: Sliding Window is a technique used to reduce nested loops into a single loop by maintaining a window (range) over data.

ğŸ‘‰ Simple words:
Ek chhota sa frame (window) data par slide karta rehta hai.

2ï¸âƒ£ Why It Is Used?
Reduces time complexity
Avoids recalculating same values
Converts O(nÂ²) â†’ O(n) in many problems
Example: Instead of checking every subarray again and again, we update only the changed element.

3ï¸âƒ£ Fixed Window vs Variable Window
ğŸ”¹ Fixed Window
Window size fixed hota hai (like size K).
Example: Maximum sum subarray of size K.
ğŸ”¹ Variable Window
Window size change hota hai condition ke hisaab se.
Example: Longest substring without repeating characters.

4ï¸âƒ£ Time Complexity Idea
Without Sliding Window â†’ O(nÂ²)
With Sliding Window â†’ O(n)
ğŸ‘‰ Less loops = faster code.

âœ… Example 1: Maximum Sum Subarray of Size K
Python
arr = [2, 1, 5, 1, 3, 2]
k = 3

window_sum = sum(arr[:k])
max_sum = window_sum

for i in range(k, len(arr)):
    window_sum = window_sum + arr[i] - arr[i-k]
    max_sum = max(max_sum, window_sum)

print("Maximum Sum:", max_sum)

âœ… Example 2: Longest Substring Without Repeating Characters
Python
s = "abcabcbb"
left = 0
seen = set()
max_length = 0

for right in range(len(s)):
    while s[right] in seen:
        seen.remove(s[left])
        left += 1
    seen.add(s[right])
    max_length = max(max_length, right - left + 1)

print("Longest Length:", max_length)

ğŸ”¥ Practice Question
If array = [1,2,3,4,5] and K = 2
What is maximum sum subarray?

ğŸ” Quick Revision (DSA)
ğŸ”¹Sliding Window reduces time complexity
ğŸ”¹Fixed window â†’ size constant
ğŸ”¹Variable window â†’ size changes
ğŸ”¹Two-pointer approach helpful
ğŸ”¹Used in substring / subarray problems

ğŸ“Œ SLOT 02: Support Vector Machine (SVM â€“ ML)

1ï¸âƒ£ What is SVM?
SVM is a supervised learning algorithm used for classification and regression.
ğŸ‘‰ Main idea: Separate data using the best possible line.

2ï¸âƒ£ Classification Concept
Example: Marks â†’ Pass / Fail
Height, Weight â†’ Category
Model learns boundary between classes.

3ï¸âƒ£ Hyperplane Idea
Hyperplane = Decision boundary line.
In 2D â†’ Line
In 3D â†’ Plane
SVM tries to find: ğŸ‘‰ The line that separates classes best.

4ï¸âƒ£ Margin Concept
Margin = Distance between hyperplane and nearest data points.
SVM tries to: ğŸ‘‰ Maximize margin.
Bigger margin = Better generalization.

5ï¸âƒ£ Kernel Trick (Basic Idea)
Sometimes data is not linearly separable.
Kernel helps to: ğŸ‘‰ Convert data into higher dimension
So it becomes separable.
Common kernels:
Linear
Polynomial
RBF
No deep math needed now.

âœ… Mini Example â€“ Train SVM
Python
from sklearn import svm

# Small dataset
X = [[2, 3], [1, 1], [2, 1], [3, 2]]
y = [1, 0, 0, 1]

model = svm.SVC(kernel='linear')
model.fit(X, y)

prediction = model.predict([[2,2]])
print("Prediction:", prediction)

ğŸ” Compare with Logistic Regression
Logistic Regression â†’ Probability based
SVM â†’ Margin maximization
SVM better for small datasets sometimes
ğŸ”¥ Practice Question
What happens if margin is very small?
Is model strong or weak?

Day 12 complete âœ…ğŸš€
